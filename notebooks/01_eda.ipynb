{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 0 - Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Environment Set Up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -qq -r ../requirements.txt\n",
    "\n",
    "# Set up root directory\n",
    "import sys\n",
    "\n",
    "REL_PATH_TO_ROOT = \"../\"\n",
    "\n",
    "sys.path.insert(0,REL_PATH_TO_ROOT)\n",
    "\n",
    "from src.utils import get_root_dir, test_root_dir\n",
    "from local_variables import ROOT_DIR\n",
    "\n",
    "test_root_dir(REL_PATH_TO_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get project questions\n",
    "from questions.questions import questions as QQQQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project specific packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant files for AirBnB\n",
    "files_of_interest = [\"calendar.csv\",\"listings.csv\"]\n",
    "years = [\"2023\",\"2024\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate for 2023 and 2024\n",
    "cal_df = pd.concat([pd.read_csv(f\"{get_root_dir()}/data/{year}_tokyo/calendar.csv\") for year in years],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate for 2023 and 2024\n",
    "list_df = pd.concat([pd.read_csv(f\"{get_root_dir()}/data/{year}_tokyo/listings.csv\") for year in years],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_bloom = pd.read_csv(f\"{get_root_dir()}/data/cherry_blossom_dates/sakura_first_bloom_dates.csv\")\n",
    "first_bloom.columns = first_bloom.columns.str.lower().str.replace(\" \",\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_bloom = pd.read_csv(f\"{get_root_dir()}/data/cherry_blossom_dates/sakura_full_bloom_dates.csv\")\n",
    "full_bloom.columns = full_bloom.columns.str.lower().str.replace(\" \",\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out questions\n",
    "def get_question(question_num):\n",
    "    print(QQQQ[question_num])\n",
    "\n",
    "for i in range(1,5):\n",
    "    get_question(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# 1 - Initial EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 1.1 - High-Level View - Calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### 1.1.1 - Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Each row corresponds to one data and one property based on the listing id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of unique properties\n",
    "cal_df[\"listing_id\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### 1.1.2 - Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time range\n",
    "cal_df[\"date_dt\"] = pd.to_datetime(cal_df[\"date\"])\n",
    "cal_df[\"date_dt\"].dt.to_period('M').value_counts().sort_index().plot(kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jump in counts for 2024-07, are there duplicates?\n",
    "dropped_dups = cal_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_dups.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No duplicates found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### 1.1.3 - Price Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn price string into a number\n",
    "cal_df[\"price_num\"] = cal_df[\"price\"].str.replace(\"$\",\"\")\n",
    "cal_df[\"price_num\"] = cal_df[\"price_num\"].str.replace(\",\",\"\")\n",
    "cal_df[\"price_num\"] = cal_df[\"price_num\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution (use log scale due to wide variation of values)\n",
    "sns.histplot(data=cal_df,x=\"price_num\",log_scale=True,bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### 1.1.4 - Average Prices Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_x_time_plot(df, datefield, x, show_percentiles=False, percentile=0.95, group_var=None, log_scale=True):\n",
    "    \"\"\"\n",
    "    Plots the average of a specified column (`x`) over time, with options for percentile bounds \n",
    "    and grouping by an additional variable.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame containing the data to be plotted.\n",
    "    datefield : str\n",
    "        The name of the column representing dates.\n",
    "    x : str\n",
    "        The column for which averages are computed and plotted.\n",
    "    show_percentiles : bool, optional, default=False\n",
    "        Whether to display percentile bounds on the plot. \n",
    "        If True, shaded areas showing percentile bounds are added.\n",
    "    percentile : float, optional, default=0.95\n",
    "        The desired percentile range to display if `show_percentiles` is True. \n",
    "        For example, 0.95 corresponds to a 95% percentile range.\n",
    "    group_var : str or None, optional, default=None\n",
    "        An optional column for grouping the data. Separate lines and percentile bounds are plotted \n",
    "        for each unique value in this column if provided.\n",
    "    log_scale : bool, optional, default=True\n",
    "        Whether to apply a natural logarithm transformation to the averages and bounds \n",
    "        before plotting.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    None\n",
    "        The function generates and displays the plot.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - If `log_scale` is True, the y-axis will represent the log-transformed values of `x`.\n",
    "    - When `show_percentiles` is True and `group_var` is provided, separate shaded areas \n",
    "      representing the percentile range are plotted for each group.\n",
    "    - The function uses Seaborn for line plotting and Matplotlib for additional styling.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    average_x_time_plot(\n",
    "        df=my_data, \n",
    "        datefield=\"date\", \n",
    "        x=\"value\", \n",
    "        show_percentiles=True, \n",
    "        percentile=0.9, \n",
    "        group_var=\"category\", \n",
    "        log_scale=False\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy dataframe to avoid making changes\n",
    "    work_df = df.copy(deep=False)\n",
    "\n",
    "    # Convert data column to DT format\n",
    "    work_df[datefield] = pd.to_datetime(work_df[datefield])\n",
    "\n",
    "    # Set grouping variables depending on inputs\n",
    "    if group_var == None:\n",
    "        group_fields = datefield\n",
    "    else:\n",
    "        group_fields = [datefield,group_var]\n",
    "\n",
    "    # Get the summary stats depending on whether upper or lower bounds were wanted\n",
    "    if show_percentiles:\n",
    "        perc = 1-percentile\n",
    "        summary_stats = work_df.groupby(group_fields)[x].agg(avg='mean',lower_bound=lambda x: x.quantile(perc/2),\n",
    "    upper_bound=lambda x: x.quantile(1-perc/2)).reset_index()\n",
    "    else:\n",
    "        summary_stats = work_df.groupby(group_fields)[x].agg(avg='mean').reset_index()\n",
    "\n",
    "    # Create necessary transformed variables depending on log_scale argument \n",
    "    if log_scale:\n",
    "        summary_stats[f\"log_{x}\"] = np.log(summary_stats['avg'])\n",
    "        if show_percentiles:\n",
    "            summary_stats['log_lower_bound'] = np.log(summary_stats['lower_bound'])\n",
    "            summary_stats['log_upper_bound'] = np.log(summary_stats['upper_bound'])\n",
    "\n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Create a palette to keep colouring of bounds consistent with the line plot\n",
    "    sns_palette = sns.color_palette(n_colors=summary_stats[group_var].nunique() if group_var else 1)\n",
    "\n",
    "    # Plot lineplot of date against (log) of average x over time\n",
    "    sns.lineplot(data=summary_stats, x=summary_stats[datefield],y=f\"log_{x}\" if log_scale else 'avg',hue=group_var, linewidth=2,palette=sns_palette)\n",
    "\n",
    "    # Create bound plots if requested\n",
    "    if show_percentiles and group_var:\n",
    "        for color, (group, group_data) in zip(sns_palette, summary_stats.groupby(group_var)):\n",
    "            plt.fill_between(\n",
    "                group_data[datefield],\n",
    "                group_data['log_lower_bound'] if log_scale else group_data['lower_bound'],\n",
    "                group_data['log_upper_bound'] if log_scale else group_data['upper_bound'],\n",
    "                alpha=0.2,\n",
    "                color=color,\n",
    "                label=f'{group} {100 * percentile}% Percentile Range'\n",
    "            )\n",
    "\n",
    "    elif show_percentiles:  # For no group_var, single fill_between\n",
    "        plt.fill_between(\n",
    "            summary_stats[datefield],\n",
    "            summary_stats['log_lower_bound'] if log_scale else summary_stats['lower_bound'],\n",
    "            summary_stats['log_upper_bound'] if log_scale else summary_stats['upper_bound'],\n",
    "            alpha=0.3,\n",
    "            color=sns_palette[0],\n",
    "            label=f'{100 * percentile}% Percentile Range'\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(f'Log ')\n",
    "    plt.title(f'Average {'Log ' if log_scale else \"\"}{x.capitalize()} Over Time with {100*percentile}% Percentile Bounds')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average log price over time\n",
    "average_x_time_plot(cal_df,\"date\",\"price_num\",show_percentiles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Huge variability in the price at a given time, what determines the price?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at availability for price \n",
    "average_x_time_plot(cal_df,\"date\",\"price_num\",show_percentiles=True,group_var=\"available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Doesn't seem to make a huge difference, although unoccupied properties tend to be priced lower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at availability for price \n",
    "plot_1_df = cal_df[cal_df[\"maximum_nights\"] <= 31]\n",
    "plot_2_df = cal_df[~(cal_df[\"maximum_nights\"] <= 31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_1_df['maximum_nights_group'] = pd.qcut(plot_1_df['maximum_nights'], q=4,duplicates=\"drop\")\n",
    "average_x_time_plot(plot_1_df,\"date\",\"price_num\",show_percentiles=False,group_var=\"maximum_nights_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Seems that smaller maximum stays are generally more expensive, while maximum stays after a certain point are more comparable. Huge drop and rebound in 2024-07 may correspond to the large depreciation of the Yen (-8% agains the dollar) which occurred early July\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2_df['maximum_nights_group'] = pd.qcut(plot_2_df['maximum_nights'], q=5,duplicates=\"drop\")\n",
    "average_x_time_plot(plot_2_df,\"date\",\"price_num\",show_percentiles=False,group_var=\"maximum_nights_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Properties with a likely placeholder maximum stay seem to have a precipitous decline for projected prices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_1_df = cal_df[cal_df[\"minimum_nights\"] <= 15]\n",
    "plot_1_df['minimum_nights_group'] = pd.qcut(plot_1_df['minimum_nights'], q=3,duplicates=\"drop\")\n",
    "average_x_time_plot(plot_1_df,\"date\",\"price_num\",show_percentiles=False,group_var=\"minimum_nights_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Seems to be a difference but not huge in size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2_df = cal_df[~(cal_df[\"minimum_nights\"] <= 15)]\n",
    "plot_2_df['minimum_nights_group'] = pd.qcut(plot_2_df['minimum_nights'], q=3,duplicates=\"drop\")\n",
    "average_x_time_plot(plot_2_df,\"date\",\"price_num\",show_percentiles=False,group_var=\"minimum_nights_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Seems to be a huge difference for before 07\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## 1.2 - Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View columns in the listings dataframe which provides more information on individual hosts \n",
    "list_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique to id and last_scraped level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### 1.2.1 - Time Structure in Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of ids\n",
    "test_ids = list_df[\"id\"].head().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how things change over time\n",
    "list_df[list_df[\"id\"].isin(test_ids)].sort_values(by=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many listings have data for both years?\n",
    "list_df[\"id\"].value_counts().reset_index()[\"count\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Around 50% only appear for one year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create average price table \n",
    "average_prices = cal_df[[\"listing_id\",\"price_num\"]].groupby(by=\"listing_id\",as_index=False).agg(\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "### 1.2.2 - Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram for review rating\n",
    "list_df[\"review_scores_rating\"].hist(bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take log to spread out distribution a bit more, add offset for reviews of 0\n",
    "np.log(list_df[\"review_scores_rating\"]+0.1).hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df[\"review_scores_rating\"].isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "stars_and_prices = pd.merge(left=average_prices,right=list_df[[\"id\",\"review_scores_rating\"]].rename(columns={\"id\":\"listing_id\"}),how=\"inner\",on=\"listing_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=stars_and_prices,x=\"review_scores_rating\",y=\"price_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove outliers on price\n",
    "\n",
    "lower_quantile = 0.05\n",
    "upper_quantile = 0.95\n",
    "\n",
    "# Calculate the lower and upper bounds\n",
    "price_lower = stars_and_prices['price_num'].quantile(lower_quantile)\n",
    "price_upper = stars_and_prices['price_num'].quantile(upper_quantile)\n",
    "\n",
    "# Filter the data to remove outliers\n",
    "filtered_data = stars_and_prices[\n",
    "    (stars_and_prices['price_num'] >= price_lower) &\n",
    "    (stars_and_prices['price_num'] <= price_upper)\n",
    "]\n",
    "\n",
    "\n",
    "sns.scatterplot(data=filtered_data,x=\"review_scores_rating\",y=\"price_num\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Still quite a lot of variability, hard to spot a trend from the visualisation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "### 1.2.3 - Room Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "roomgen_and_price = pd.merge(left=average_prices,right=list_df[[\"id\",\"room_type\"]].rename(columns={\"id\":\"listing_id\"}),how=\"inner\",on=\"listing_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "### 1.2.4 - Number of Rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_room_df = pd.merge(left=average_prices,right=list_df[[\"id\",\"bedrooms\"]].rename(columns={\"id\":\"listing_id\"}),how=\"inner\",on=\"listing_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc=0.05\n",
    "price_by_room_df = price_room_df[[\"bedrooms\",\"price_num\"]].groupby(by=\"bedrooms\",as_index=True)[\"price_num\"].agg(avg_price=\"mean\",lower_bound=lambda x: x.quantile(perc/2),\n",
    "    upper_bound=lambda x: x.quantile(1-perc/2)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_by_room_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=price_by_room_df.sort_values(by=\"bedrooms\",ascending=True),x=\"bedrooms\",y=\"avg_price\")\n",
    "sns.barplot(data=price_by_room_df.sort_values(by=\"bedrooms\",ascending=True),x=\"bedrooms\",y=\"lower_bound\",color='r',alpha=0.3)\n",
    "sns.barplot(data=price_by_room_df.sort_values(by=\"bedrooms\",ascending=True),x=\"bedrooms\",y=\"upper_bound\",color='g',alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bedrooms seems to have some correlation to price for less than 5 bedrooms, perhaps 6 bedrooms represent hostels etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df[\"bedrooms\"].value_counts().reset_index().sort_values(by=\"bedrooms\",ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "### 1.2.5 - Neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df[\"neighbourhood\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df[\"neighbourhood_cleansed\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhood_price_df = pd.merge(left=average_prices,right=list_df[[\"id\",\"neighbourhood_cleansed\"]].rename(columns={\"id\":\"listing_id\"}),how=\"inner\",on=\"listing_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_by_neigh_df = neighbourhood_price_df.groupby(\"neighbourhood_cleansed\",as_index=False).agg(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=price_by_neigh_df.sort_values(by=\"price_num\",ascending=False),x=\"neighbourhood_cleansed\",y=\"price_num\",hue=\"neighbourhood_cleansed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Neighbourhood seems to have a big impact on price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "### 1.2.6 - All Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "types_df = list_df.dtypes.reset_index()\n",
    "types_df.columns = [\"col\",\"type\"]\n",
    "types_df[\"type\"] = types_df[\"type\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = types_df[types_df[\"type\"].apply(lambda x : True if (\"int\" in x or \"float\" in x) else False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars[\"na_rate\"] = num_vars[\"col\"].apply(lambda x : list_df[x].isna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=num_vars.sort_values(by=\"na_rate\",ascending=False),y=\"na_rate\",x=\"col\")\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_var_cols = num_vars[\"col\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pd.merge(left=list_df[num_var_cols],right=average_prices.rename(columns={\"listing_id\" : \"id\"}),how=\"inner\",on=\"id\")\n",
    "corr_df.drop(columns=[\"id\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations\n",
    "correlations = corr_df.corr()['price_num'].drop('price_num')  # Exclude price\n",
    "\n",
    "# Plot bar chart\n",
    "correlations.plot(kind='bar', title='Correlations with Price')\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('Correlation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "### 1.2.6 - All Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse of the num_vars condition\n",
    "cat_vars = types_df[types_df[\"type\"].apply(lambda x : False if (\"int\" in x or \"float\" in x) else True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example values\n",
    "for col in cat_vars[\"col\"].values:\n",
    "    print(f\"\"\"\n",
    "    {col}\n",
    "    {list_df[col].sample(n=1,random_state=42).iloc[0]}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which fields are redundant i.e. links or names, non-comparable data or free-text?\n",
    "redundant_fields = [\"listing_url\",\n",
    "                    \"name\"\n",
    "                    \"source\",\n",
    "                    \"description\",\n",
    "                    \"host_url\",\n",
    "                    \"host_name\",\n",
    "                    \"host_about\",\n",
    "                    \"host_thumbnail_url\",\n",
    "                    \"host_picture_url\",\n",
    "                    \"license\",\n",
    "                    \"picture_url\",\n",
    "                    \"name\",\n",
    "                    \"neighborhood_overview\",\n",
    "                    \"last_scraped\",\n",
    "                   \"first_review\",\n",
    "                   \"last_review\",\n",
    "                   \"calendar_last_scraped\",\n",
    "                    \"host_since\",\n",
    "                    \"price\",\n",
    "                    \"amenities\"\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = cat_vars[cat_vars[\"col\"].apply(lambda x : True if x not in redundant_fields else False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = pd.get_dummies(list_df[[\"price\"] + cat_cols[\"col\"].to_list()], columns=cat_cols[\"col\"].to_list(),drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "### 1.2.7 - Running A Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaNs for modelling\n",
    "model_df = cat_df.dropna(subset=\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df[\"price_transformed\"] = model_df[\"price\"].apply(lambda x : float(x.replace(\",\",\"\").replace(\"$\",\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_df[[col for col in model_df.columns if col not in [\"price\",\"price_transformed\"]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= model_df[\"price_transformed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"R²: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model performs terribly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "## 1.3 - Cherry Blossom Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "### 1.3.1 - High Level View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_bloom.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_bloom.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locations covered\n",
    "first_bloom[\"site_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare to neighbourhoods \n",
    "neighbourhoods = list_df[\"neighbourhood_cleansed\"].unique()\n",
    "bloom_sites = first_bloom[\"site_name\"].unique()\n",
    "\n",
    "\n",
    "# Make into single strings\n",
    "neighbourhoods = [neighbourhood.split()[0] for neighbourhood in list_df[\"neighbourhood_cleansed\"].unique()]\n",
    "bloom_sites = [site.split()[0] for site in first_bloom[\"site_name\"].unique()]\n",
    "\n",
    "# Find crossover \n",
    "intersection = set(neighbourhoods).intersection(set(bloom_sites))\n",
    "print(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the 'Tokyo Japan' area covers our AirBnB data\n",
    "tokyo_first_bloom = first_bloom[first_bloom[\"site_name\"] == \"Tokyo Japan\"]\n",
    "tokyo_full_bloom = full_bloom[full_bloom[\"site_name\"] == \"Tokyo Japan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokyo_first_bloom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokyo_full_bloom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Does not cover out date range, can we find the dates elswhere?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"Using :{https://www.jrailpass.com/blog/japan-cherry-blossom-forecast}\n",
    "\n",
    "first bloom is 2024-03-29 and full bloom is 2024-04-04\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "near_to_bloom_cal_df = cal_df[(pd.to_datetime(cal_df[\"date\"]) < '2024-05-01') & (pd.to_datetime(cal_df[\"date\"]) > '2024-02-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at availability for price \n",
    "average_x_time_plot(near_to_bloom_cal_df,\"date\",\"price_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Seems to be a change, can we check using the Mann-Whitney U test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "# 2 - Answering Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {},
   "source": [
    "## 2.1 - Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_question(\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122",
   "metadata": {},
   "source": [
    "### 2.1.1 - Try A Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take smaller sample to minimise data set size \n",
    "print(list_df[\"id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a stratified sample\n",
    "\n",
    "# Define sample fraction (e.g., 50% from each group)\n",
    "sample_fraction = 0.2\n",
    "\n",
    "# Perform stratified sampling\n",
    "stratified_sample = list_df.groupby('neighbourhood_cleansed', group_keys=False).apply(lambda x: x.sample(frac=sample_fraction,random_state=42))\n",
    "ids = list(stratified_sample[\"id\"].unique())\n",
    "print(f\"Now there are {len(ids)} ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get neighbourhood information\n",
    "q1_df = pd.merge(left=cal_df[cal_df[\"listing_id\"].isin(ids)].rename(columns={\"listing_id\" : \"id\"}),right=list_df[list_df[\"id\"].isin(ids)][[\"id\",\"neighbourhood_cleansed\"]],how=\"inner\",on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn neighbourhood into categorical columns \n",
    "q1_df = pd.get_dummies(q1_df,columns=[\"neighbourhood_cleansed\"],drop_first=True,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove nans for prediction variable\n",
    "q1_df = q1_df.dropna(subset=\"price_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove prices of 0 before logging \n",
    "q1_df = q1_df[q1_df[\"price_num\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = q1_df[[col for col in q1_df.columns if \"neighbourhood_cleansed\" in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log(q1_df[\"price_num\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhood_model = LinearRegression()\n",
    "neighbourhood_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = neighbourhood_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"R²: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"It doesn't predict it very well, what about if we just look at average price?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139",
   "metadata": {},
   "source": [
    "### 2.2.2 - Simplified Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_avg_df = list_df[[\"neighbourhood_cleansed\",\"price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_avg_df[\"price_num\"] = q1_avg_df[\"price\"].apply(lambda x : str(x).replace(\",\",\"\").replace(\"$\",\"\")).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_avg_df = q1_avg_df.dropna(subset=\"price_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_avg_df = q1_avg_df[q1_avg_df[\"price_num\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_avg_df[\"log_price\"] = np.log(q1_avg_df[\"price_num\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_avg_df = pd.get_dummies(q1_avg_df,columns=[\"neighbourhood_cleansed\"],drop_first=True,dtype=int).dropna(subset=\"log_price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_avg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = q1_avg_df[[col for col in q1_avg_df.columns if \"neighbourhood_cleansed\" in col]]\n",
    "y = q1_avg_df[\"log_price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpler_neighbourhood_model = LinearRegression()\n",
    "simpler_neighbourhood_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = simpler_neighbourhood_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"R²: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Still performs poorly, other features are clearly important\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153",
   "metadata": {},
   "source": [
    "## 2.2 - Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154",
   "metadata": {},
   "source": [
    "### 2.2.1 - Performing Significance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
